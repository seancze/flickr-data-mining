{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flickrapi\n",
    "# !pip install geopy\n",
    "# !pip install python-datamuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flickrapi\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import datamuse\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.geocoders import GoogleV3\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "api_key = '1107afb83cb2dc63722f34bf5257f908'\n",
    "secret = '589b61bab623b7ba'\n",
    "# flickr_api.set_keys(api_key = api_key, api_secret = secret)\n",
    "\n",
    "flickr = flickrapi.FlickrAPI(api_key, secret, format = 'parsed-json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top3_tags(main_tag):\n",
    "    '''\n",
    "    Input: A main tag with a type of String\n",
    "    \n",
    "    Output: Returns the top 3 tags for each cluster that is related to the main tag inputted.\n",
    "    '''\n",
    "    top3_tags = []\n",
    "    \n",
    "    cluster_list = flickr.tags.getClusters(tag = main_tag)[\"clusters\"][\"cluster\"]\n",
    "    tag_list = cluster_list[1][\"tag\"][0:3]\n",
    "\n",
    "    for cluster in cluster_list:\n",
    "        tag_list = cluster[\"tag\"][0:3]\n",
    "        top3_tags_per_cluster = []\n",
    "        for tag in tag_list:\n",
    "            top3_tags_per_cluster.append(tag.get(\"_content\"))\n",
    "        top3_tags.append(top3_tags_per_cluster)\n",
    "    return top3_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ids(main_tag, top3_tags):\n",
    "    '''\n",
    "    Returns a list of user_ids for the top 24 photos returned based on the inputted tag and cluster tags related to it\n",
    "    '''\n",
    "    user_id_and_tags = {}\n",
    "    start_time = time.time()\n",
    "    user_ids = []\n",
    "#     Obtain the corresponding list of main and cluster tags used to find the user\n",
    "    for tags in top3_tags:\n",
    "        \n",
    "        try:\n",
    "            tags_to_string = '-'.join(tags)\n",
    "            user_list = flickr.tags.getClusterPhotos(tag=main_tag, cluster_id=tags_to_string)[\"photos\"][\"photo\"]\n",
    "        except:\n",
    "            pass\n",
    "        for user in user_list:\n",
    "            user_id_and_tags['main_tag'] = main_tag\n",
    "            user_id_and_tags['cluser_tags'] = tags_to_string\n",
    "            user_id_and_tags['owner'] = user['owner']\n",
    "            user_ids.append(user_id_and_tags.copy())\n",
    "    print(f\"Number of user_ids: {len(user_ids)}\")\n",
    "    return user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_from_groups(group_ids):\n",
    "    '''\n",
    "    Input: A list of group_ids\n",
    "    Output: A list of user_ids found in the group. Only works for groups with public access\n",
    "    '''\n",
    "    member_ids = []\n",
    "    all_pages = []\n",
    "#     Obtain user_ids for all members in a list of group_ids\n",
    "    for group in group_ids:\n",
    "        try:\n",
    "            num_pages = flickr.groups.members.getList(group_id = group, per_page = 500, page = 1)[\"members\"][\"pages\"]\n",
    "            for page in range(num_pages):\n",
    "                all_pages.append(flickr.groups.members.getList(group_id = group, page = page))\n",
    "                member_list = flickr.groups.members.getList(group_id = group, per_page = 500, page = page)[\"members\"][\"member\"]\n",
    "                for member in member_list:\n",
    "                    member_ids.append(member[\"nsid\"])\n",
    "        except:\n",
    "            print(\"Error in getting members from Group\")\n",
    "            pass\n",
    "    print(f'User_ids from groups: {len(member_ids)}')\n",
    "    return member_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_users(user_ids):\n",
    "    '''\n",
    "    Returns a list of user_ids who have been on Flickr for more than 2 years and have posted more than 1000 photos\n",
    "    '''\n",
    "    \n",
    "    start_time = time.time()\n",
    "    shortlisted_ids = []\n",
    "    for user_id in user_ids:\n",
    "\n",
    "#         Obtain the metadata related to a Flicker's user photographs\n",
    "        try:\n",
    "            photos_dictionary = flickr.people.getInfo(user_id=user_id[\"owner\"])[\"person\"][\"photos\"]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "#     Retrieve number of pictures\n",
    "        num_pictures = photos_dictionary[\"count\"][\"_content\"]\n",
    "    \n",
    "#     Retrieve date of first photo as a string and convert to date object\n",
    "        date_str = photos_dictionary[\"firstdatetaken\"][\"_content\"]\n",
    "\n",
    "        if date_str != None:\n",
    "            if date_str.split('-')[0] != '0000':\n",
    "                first_photo_date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "            elif date_str.split('-')[0] == '0000':\n",
    "                print(\"Year is 0000\")\n",
    "        elif date_str == None:\n",
    "            print(\"Date field is empty\")\n",
    "            \n",
    "        \n",
    "        two_years_ago = datetime.now() - relativedelta(years=2)\n",
    "    #     If more than 2 years AND > 1000 photos, add user_id\n",
    "        \n",
    "        if two_years_ago > first_photo_date and num_pictures > 1000:\n",
    "            shortlisted_ids.append(user_id)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(f\"Shortlisted_ids: {len(shortlisted_ids)}\")\n",
    "    return shortlisted_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globetrotters(all_tags, file_name):\n",
    "    '''\n",
    "    Input:\n",
    "    1) A list of tags\n",
    "    2) A string to name your .json file\n",
    "    \n",
    "    Output:\n",
    "    1) A list of unique user ids with more than 1,000 photos and who have been on Flickr for > 2 years\n",
    "    2) A .json file containing all the user's information according to flickr's API (flickr.people.getInfo)\n",
    "    '''\n",
    "    \n",
    "    finalised_user_info = []\n",
    "    finalised_user_ids = []\n",
    "    finalised_user_ids_sorted = []\n",
    "#     Loop through all tags\n",
    "\n",
    "    \n",
    "# 1st way of retrieving user ids: By selecting a few tags related to travel and automatically finding related tags and photos with these tags\n",
    "# Obtain the user_ids from these pictures\n",
    "    for tag in all_tags:\n",
    "        \n",
    "#         To allow the loop to continue running in the event any tag causes any error in one of the 3 functions\n",
    "        try:\n",
    "            top3_tags = get_top3_tags(tag)\n",
    "\n",
    "            user_ids = get_user_ids(tag, top3_tags)\n",
    "            \n",
    "#             user_ids = user_ids_1 + user_ids_from_groups\n",
    "            \n",
    "            shortlisted_ids = filter_users(user_ids)\n",
    "\n",
    "    #         If user_id has NOT already been appended, then append to finalised_user_ids\n",
    "            for potential_user_id in shortlisted_ids:\n",
    "                if not any(user['owner'] == potential_user_id['owner'] for user in finalised_user_ids):\n",
    "                    finalised_user_ids.append(potential_user_id)\n",
    "\n",
    "            print(f'Total user_ids: {len(finalised_user_ids)}')\n",
    "            \n",
    "        except:\n",
    "            print('Error. Moving over to next loop.')\n",
    "            pass\n",
    "\n",
    "#         Get all user_info from the respective user_ids\n",
    "    for user_id in finalised_user_ids:\n",
    "        try:\n",
    "            finalised_user_info.append(flickr.people.getInfo(user_id=user_id[\"owner\"]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "#     Sort user_info\n",
    "    finalised_user_info_sorted = sorted(finalised_user_info, key = lambda i : i[\"person\"][\"photos\"][\"count\"][\"_content\"], reverse = True)\n",
    "# Sort user_ids according to the sorted user_info above\n",
    "    for user in finalised_user_info:\n",
    "        for user_id in finalised_user_ids:\n",
    "            if user_id[\"owner\"] == user[\"person\"][\"id\"]:\n",
    "                finalised_user_ids_sorted.append(user_id)\n",
    "\n",
    "        \n",
    "    with open(f'{file_name}.json', 'w') as json_file:\n",
    "        json.dump(finalised_user_info, json_file)\n",
    "\n",
    "    return finalised_user_ids_sorted, finalised_user_info_sorted\n",
    "#     finalised_user_info = list(map(lambda x : flickr.people.getInfo(user_id=x), finalised_user_ids))                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpacker, tourist, vacation, sightseeing, scenery, holiday resort, excursion, hiking, cruise, globetrotter, adventurer, mountaineer, hotel, amusement park, ryokan, festival, carnival\n",
    "# The list of tags manually determined by a person to obtain related photos\n",
    "main_tags = ['backpacker', 'tourist', 'vacation', 'sightseeing', 'scenery', 'holiday resort', 'excursion', 'hiking', 'cruise', 'globetrotter', 'adventurer', 'mountaineer', 'hotel', 'amusement park', 'ryokan', 'festival', 'carnival']\n",
    "main_tags_2 = ['backpacker', 'tourist', 'adventurer', 'globetrotter', 'mountaineer', 'traveller']\n",
    "\n",
    "def get_related_tags(main_tags):\n",
    "    '''\n",
    "    Returns top 10 related words according to datamuse API\n",
    "    '''\n",
    "    api = datamuse.Datamuse()\n",
    "    all_tags = []\n",
    "    for tag in main_tags:\n",
    "        words = api.words(ml = tag, max=10)\n",
    "        for word in words:\n",
    "            all_tags.append(word.get('word'))\n",
    "            \n",
    "#     Remove duplicates from list\n",
    "    unique_tags = list(set(all_tags))\n",
    "    print(f'Number of tags: {len(unique_tags)}')\n",
    "    return unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all_tags\n",
    "all_tags = get_related_tags(main_tags_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain list of user_ids + user_info from all_tags (This takes awhile, like ~15-30min?)\n",
    "finalised_user_ids, finalised_user_info = globetrotters(all_tags, \"user_info_tags_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finalised_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_headers(user_ids, user_info_list, file_name):\n",
    "    '''\n",
    "    Input: \n",
    "    - A list of user_ids. Each user_id should be a dictionary returned from globetrotters()\n",
    "    - A list of user_info. Each user_info should be a dictionary returned from globetrotters()\n",
    "    - Name of file. Note: MUST end in .csv as it has to be a .csv file!\n",
    "    \n",
    "    Output: A table with the following headers\n",
    "    - main_tag\n",
    "    - cluster_tags\n",
    "    - user_id\n",
    "    - username\n",
    "    - location\n",
    "    - num_photos\n",
    "    - date_of_first_photo\n",
    "    '''\n",
    "    f = csv.writer(open(f\"{file_name}.csv\", \"w+\", encoding = \"utf-8\"))\n",
    "\n",
    "\n",
    "\n",
    "    # Write CSV Header, If you dont need that, remove this line\n",
    "    f.writerow([\"main_tag\", \"cluser_tags\", \"user_id\", \"username\", \"location\", \"num_photos\", \"date_of_first_photo\"])\n",
    "\n",
    "    for i, el in enumerate(user_ids):\n",
    "    #     Try retrieving location as location may be None\n",
    "        try:\n",
    "            location = user_info_list[i][\"person\"][\"location\"][\"_content\"]\n",
    "        except:\n",
    "            location = None\n",
    "        f.writerow([user_ids[i][\"main_tag\"],\n",
    "                    user_ids[i][\"cluser_tags\"],\n",
    "                    user_ids[i][\"owner\"],\n",
    "                    user_info_list[i][\"person\"][\"username\"][\"_content\"],\n",
    "                    location,\n",
    "                    user_info_list[i][\"person\"][\"photos\"][\"count\"][\"_content\"],\n",
    "                    user_info_list[i][\"person\"][\"photos\"][\"firstdatetaken\"][\"_content\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_relevant_headers(finalised_user_ids, finalised_user_info, \"Relevant User Info 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be ignored:\n",
    "# Just to dump user_info_from_groups into a separate .json file because there are too many user_ids from groups\n",
    "# unique_ids_from_group\n",
    "# finalised_user_info_from_groups = []\n",
    "# for user_id in shortlisted_ids_2:\n",
    "#     try:\n",
    "#         finalised_user_info_from_groups.append(flickr.people.getInfo(user_id=user_id))\n",
    "#     except:\n",
    "#         print(f'''\n",
    "#         Error getting user info.\n",
    "#         Current user_info length: {len(finalised_user_info_from_groups)}\n",
    "#         ''')\n",
    "#         pass\n",
    "\n",
    "# with open(f'user_info_from_3_travel_groups.json', 'w') as json_file:\n",
    "#     json.dump(finalised_user_info_from_groups, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain user_ids from .txt file\n",
    "def retrieve_user_ids(path_as_string):\n",
    "    a_file = open(path_as_string, \"r\")\n",
    "\n",
    "    list_of_user_ids = []\n",
    "    for line in a_file:\n",
    "\n",
    "        stripped_line = line.strip()\n",
    "        list_of_user_ids.append(stripped_line)\n",
    "\n",
    "    a_file.close()\n",
    "    return list_of_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_user_ids = retrieve_user_ids(path_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_location(photo, is_free = True, google_api_key = ''):\n",
    "    '''\n",
    "    Retrieves location of a single photo via Nominatim (OpenStreetMaps)\n",
    "    Note: Choice of geocoder (A) Nominatim & OpenStreetMaps (B) Google Maps\n",
    "    If (A), it is slower.\n",
    "    If (B), requires Google API Key\n",
    "    '''\n",
    "    lat = photo.get(\"latitude\")\n",
    "    lon = photo.get(\"longitude\")\n",
    "    coordinates = f\"{lat}, {lon}\"\n",
    "    \n",
    "#     If image is NOT geotagged, return NIL\n",
    "    if lat == 0 and lon == 0:\n",
    "        return \"NIL\"\n",
    "    \n",
    "#     Check if using free geocoder or not\n",
    "    if is_free:\n",
    "        locator = Nominatim(user_agent=\"myGeocoder\", timeout = 10)\n",
    "    else:\n",
    "#         Checks for API Key\n",
    "        if google_api_key != '':\n",
    "            locator = GoogleV3(api_key=google_api_key)\n",
    "        else:\n",
    "            raise Exception(\"Please fill in Google API Key\")\n",
    "    location = locator.reverse(coordinates, exactly_one = True)\n",
    "    \n",
    "    if location != None:\n",
    "        return str(location)\n",
    "    else:\n",
    "        print(\"Location field is empty\")\n",
    "        return \"NIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_images_metadata(user_ids, has_geo = True, is_free = True, google_api_key = ''):\n",
    "    '''\n",
    "    Input: A list of user_ids\n",
    "    Output: A list of photo metadata\n",
    "    \n",
    "    Note: \n",
    "    1) Choice of geocoder (A) Nominatim & OpenStreetMaps (B) Google Maps\n",
    "        If (A), it is slower.\n",
    "        If (B), requires Google API Key\n",
    "    2) If has_geo = True, returns all photos which have been geo-tagged. Some location fields may still be blank.\n",
    "    If has_geo = False, returns all photos regardless of whether it has been geo-tagged.\n",
    "    '''\n",
    "#     To use flickr.walk output needs to be in 'etree', not 'parsed-json'\n",
    "    start_time = time.time()\n",
    "    flickr = flickrapi.FlickrAPI(api_key, secret, format = 'etree')\n",
    "    \n",
    "#     Each user_id is a dictionary containing user_id and images_metadata\n",
    "#     Array of user_ids. Each user_id is a dictionary?\n",
    "    \n",
    "#     A list of all user's dictionary\n",
    "    metadata_list = []\n",
    "    data = {}\n",
    "    for user_id in user_ids:\n",
    "#         A dictionary for 1 user containing the user's images' metadata\n",
    "        metadata_per_user = {\"user_id\": user_id, \"images_metadata\": []}\n",
    "        if has_geo:\n",
    "            for photo in flickr.walk(user_id = user_id, has_geo = \"1\", extras = \"geo, url_c, url_l\"):\n",
    "                data[\"url_1024\"] = photo.get(\"url_l\")\n",
    "                data[\"location\"] = retrieve_location(photo, is_free, google_api_key)\n",
    "                metadata_per_user[\"images_metadata\"].append(data.copy())\n",
    "        else:\n",
    "            for photo in flickr.walk(user_id = user_id, extras = \"geo, url_c, url_l\"):\n",
    "                data[\"url_1024\"] = photo.get(\"url_l\")\n",
    "                data[\"location\"] = retrieve_location(photo, is_free, google_api_key)\n",
    "                metadata_per_user[\"images_metadata\"].append(data.copy())\n",
    "        metadata_list.append(metadata_per_user)\n",
    "        print(f\"Total number of photos for {user_id}: {len(metadata_list[-1]['images_metadata'])}\")\n",
    "            \n",
    "    time_taken = round(time.time() - start_time, 2)\n",
    "    print(f'''\n",
    "    Total number of users: {len(metadata_list)}\n",
    "    Time Taken: {time_taken}s\n",
    "    ''')\n",
    "    \n",
    "    flickr = flickrapi.FlickrAPI(api_key, secret, format = 'parsed-json')\n",
    "    return metadata_list\n",
    "\n",
    "def download_images(folder_name, metadata_list):\n",
    "    '''\n",
    "    Input: metadata_list from the output of retrieve_images_metadata\n",
    "    Output: Downloads images as .jpg into a folder labelled 'folder_name' which will be found within an 'images' folder\n",
    "    '''\n",
    "    \n",
    "    for i, photo in enumerate(metadata_list):\n",
    "        # Download image from the url and save it to '00001.jpg'\n",
    "        directory = os.path.join(f\"{os.getcwd()}\", \"images\", f\"{folder_name}\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        if photo[\"url_1024\"] != None:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(photo[\"url_1024\"], f\"{directory}\\\\{i}.jpg\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not retrieve image, {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IN USE: Replaced by flickr.walk\n",
    "# Retrieve all photos with geo tag from page 1\n",
    "# Note: For geo queries, only 250 photos can be returned at a time\n",
    "# first_page = flickr.photos.search(user_id = \"11912022@N03\", has_geo = \"1\", extras = \"geo, url_c, url_l\")\n",
    "\n",
    "# Retrieve number of pages\n",
    "# num_pages_of_photos = first_page[\"photos\"][\"pages\"]\n",
    "# photos_per_page = first_page[\"photos\"][\"photo\"]\n",
    "\n",
    "# For each page of photos\n",
    "\n",
    "# NOT IN USE: Merged with retrieve_images_metadata\n",
    "# def retrieve_list_of_locations(list_of_user_ids, is_free = True, google_api_key = ''):\n",
    "#     flickr = flickrapi.FlickrAPI(api_key, secret, format = 'etree')\n",
    "#     locations = []\n",
    "#     for user_id in list_of_user_ids:\n",
    "# #         first_page = flickr.photos.search(user_id = user_id, has_geo = \"1\", extras = \"geo, url_c, url_l\")\n",
    "# #         num_pages_of_photos = first_page[\"photos\"][\"pages\"]\n",
    "# #         photos_per_page = first_page[\"photos\"][\"photo\"]\n",
    "        \n",
    "#         for photo in flickr.walk(user_id = \"11912022@N03\", has_geo = \"1\", extras = \"geo, url_c, url_l\"):\n",
    "#             locations.append(retrieve_location(photo, is_free, google_api_key))\n",
    "            \n",
    "#     print(f\"Total number of locations: {len(locations)}\")\n",
    "#     flickr = flickrapi.FlickrAPI(api_key, secret, format = 'parsed-json')\n",
    "#     return locations\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flickr.favorites.getList(user_id = \"185575499@N08\")\n",
    "# group_ids = ['1249780@N23', '2620781@N24', '853792@N24']\n",
    "\n",
    "# NOT IN USE: For testing purposes\n",
    "# download_images('Paris', photo_data)\n",
    "# flickr.photos.search(user_id = \"11912022@N03\", has_geo = \"1\", extras = \"geo, url_c, url_l\")\n",
    "# user_id = first_page[\"photos\"][\"photo\"][0][\"owner\"]\n",
    "# def flickrwalk():\n",
    "#     photos = []\n",
    "#     flickr = flickrapi.FlickrAPI(api_key, secret, format = 'etree')\n",
    "#     for photo in flickr.walk(user_id = \"89749977@N00\", has_geo = \"1\", extras = \"geo, url_c, url_l\"):\n",
    "#         photos.append(photo.get(\"url_l\"))\n",
    "#     print(len(photos))\n",
    "#     flickr = flickrapi.FlickrAPI(api_key, secret, format = 'etree')\n",
    "#     return photos\n",
    "\n",
    "# trial = flickr.photos.search(user_id = \"36891690@N06\", has_geo = \"1\", extras = \"geo, url_c, url_l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IN USE: Sample code to download images\n",
    "# import flickrapi\n",
    "# import urllib\n",
    "# from PIL import Image\n",
    "\n",
    "# # Flickr api access key \n",
    "# flickr=flickrapi.FlickrAPI('c6a2c45591d4973ff525042472446ca2', '202ffe6f387ce29b', cache=True)\n",
    "\n",
    "\n",
    "# keyword = 'siberian husky'\n",
    "\n",
    "# photos = flickr.walk(text=keyword,\n",
    "#                      tag_mode='all',\n",
    "#                      tags=keyword,\n",
    "#                      extras='url_c',\n",
    "#                      per_page=100,           # may be you can try different numbers..\n",
    "#                      sort='relevance')\n",
    "\n",
    "# urls = []\n",
    "# for i, photo in enumerate(photos):\n",
    "#     print (i)\n",
    "    \n",
    "#     url = photo.get('url_c')\n",
    "#     urls.append(url)\n",
    "    \n",
    "#     # get 50 urls\n",
    "#     if i > 50:\n",
    "#         break\n",
    "\n",
    "# print (urls)\n",
    "\n",
    "# # Download image from the url and save it to '00001.jpg'\n",
    "# urllib.urlretrieve(urls[1], '00001.jpg')\n",
    "\n",
    "# # Resize the image and overwrite it\n",
    "# image = Image.open('00001.jpg') \n",
    "# image = image.resize((256, 256), Image.ANTIALIAS)\n",
    "# image.save('00001.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
